{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b73a5c",
   "metadata": {},
   "source": [
    "# Project: Wrangling WeRateDogs Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba42562",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this project, I will be working with data from a Twitter account known as @WeRateDogs. This data will be gathered in three ways. For the first dataset I will use a downloaded archive of their 2017 tweets that the account owners downloaded and sent to Udacity in an email, the second dataset is an image prediction file that Udacity provided as part of the project, and the third dataset will be acquired through the Twitter API.\n",
    "\n",
    "The objective of this project is to demonistrate the steps taken in order to gather the data from it's sources, asses it, clean it, store it, analyze it and draw conclussions, and finally create some cool visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93586b9e",
   "metadata": {},
   "source": [
    "## Data Gathering\n",
    "\n",
    "**Datasets to gather:**\n",
    "1. A .csv file from their archive (twitter-archive-enhanced.csv)\n",
    "2. An image prediction model as a .tsv file (image-predictions.tsv)\n",
    "3. Tweets from the Twitter API that correspond to the ids in the archive file "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de71cfbf",
   "metadata": {},
   "source": [
    "### Python Libraries Used\n",
    "* json - used for reading json files\n",
    "* requests - used for downloading the image prediction file from the internet\n",
    "* os - used to access files on my disk and also to create folders and files\n",
    "* numpy as np - used to access data as arrays (assigned to an alias np)\n",
    "* pandas as pd - used for reading files into dataframes and manupilating them (assigned to an alias pd)\n",
    "* matplotlib.pyplot as plt - used to create visualizations (assigned to an alias plt)\n",
    "* seaborn as sns - used to create visualizations (assigned to an alias sns)\n",
    "* tweepy - used to query data through the Twitter API\n",
    "* default_timer as timer - used to calculate code running time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e398085",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d6cf59",
   "metadata": {},
   "source": [
    "```python\n",
    "import json\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tweepy\n",
    "from timeit import default_timer as timer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba9bd40",
   "metadata": {},
   "source": [
    "### Read the archive file from local storage\n",
    "I will use the pd.read_csv function to read the file from the local storage and then assign the resulting dataframe to a varible called **archive**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0e4892",
   "metadata": {},
   "source": [
    "```\n",
    "archive = pd.read_csv('twitter-archive-enhanced.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95faf9f",
   "metadata": {},
   "source": [
    "### Use the Requests library to download the image prediction file\n",
    "I write a generic function that accepts the file url and a folder name as parameters, that:\n",
    "* Uses the os.path.exists function to check if a folder with a given name exists on the machine\n",
    "* If a folder with the given name exists, it skips that operation\n",
    "* If a folder with the given name doesn't exist, it creates one with the given name and proceeds to:\n",
    "    * Use the request library to acquire the response from a given url\n",
    "    * Uses the with open function to open the given folder\n",
    "    * Splits the given file url string at the occurence of the '/' symbol to create a list of elements\n",
    "    * Uses the last element in the list as the file name (image-predictions.tsv)\n",
    "    * Creates the file without making any changes ('wb') in the given folder\n",
    "    * Displays an error message from the requests library if there is an error\n",
    "\n",
    "I call the function passing in the url and folder name respectively as arguments.\n",
    "\n",
    "I use pd.read_csv (passing a \\t to indicate a tab separator) to read the file into a dataframe called **predictions**.\n",
    "\n",
    "**Deliverable: All operations ran succesfully and the file was downloaded. Additionally, there was no error raised at the end of this request**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a706a633",
   "metadata": {},
   "source": [
    "```\n",
    "def read_from_url(url, folder_name):\n",
    "    \"\"\"A function that creates a folder on disk then downloads \n",
    "    data from the internet using the requests library and writes it to a file it creates \n",
    "    by splitting the url at the '/' symbol to create a list of elements in the url string\n",
    "    and grabs the last element in the list to use as the file name. \n",
    "    a creates\"\"\"\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            with open(os.path.join(folder_name, url.split('/')[-1]), mode='wb') as url_file:\n",
    "                url_file.write(response.content)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            raise SystemExit(e)\n",
    "# save file\n",
    "read_from_url('https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv', 'dog_ratings_twitter')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc4a040",
   "metadata": {},
   "source": [
    "```\n",
    "predictions = read_csv('dog_ratings_twitter/image-predictions.tsv','\\t')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54546db",
   "metadata": {},
   "source": [
    "### Use the Tweepy library to query data via the Twitter API\n",
    "* I sign up for an account at the [Twitter Developers platform](https://developer.twitter.com/en)\n",
    "* I submit an applicatin to use the Twitter API in my app\n",
    "* My application gets approved and I receive tokens to access the API! YAY!!\n",
    "\n",
    "I write a Python class called Tweepy that:\n",
    "* Declares and assigns (from the disk using os.environ.get) \n",
    "the four tokens acquired as class attributes assigned every time the class is called (This is sufficient to use with my one Twitter account) \n",
    "* Defines a class method called **authenticate that:\n",
    "    * Uses the tweepy library to authenticate my API request\n",
    "    * Sets the wait_on_rate_limit and wait_on_rate_limit_notify to True \n",
    "    in order to wait when the limit of 15 minutes is reached and to notify when waiting on rate limit.\n",
    "    * Returns an authenticated tweepy.api.API object that I will use to retrieve data using tweet ids from the archive file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dcc6c4",
   "metadata": {},
   "source": [
    "#### Tweepy class\n",
    "```\n",
    "class Tweepy:\n",
    "    '''\n",
    "    A class that takes in four api tokens from the Twitter API.\n",
    "    Defines a class method that uses the tweepy library to\n",
    "    create a tweepy.OAuthHandler object by passing\n",
    "    in the consumer key and secret declared in the class.\n",
    "    Use the created object to set the access tokens.\n",
    "    Call the tweepy.API function by passing in the authenticated object\n",
    "    and setting the wait_on_rate_limit and wait_on_rate_limit_notify to True\n",
    "    in order to wait the limit of 15 minutes is reached and to notify when waiting on rate limit.\n",
    "    Return an authenticated tweepy.api.API object used to retrieve data.\n",
    "    '''\n",
    "    consumer_key = os.environ.get('CONSUMER_KEY')\n",
    "    consumer_secret = os.environ.get('CONSUMER_SECRET')\n",
    "    access_token = os.environ.get('ACCESS_TOKEN')\n",
    "    access_secret = os.environ.get('ACCESS_SECRET')\n",
    "\n",
    "    def authenticate(self):\n",
    "        auth = tweepy.OAuthHandler(self.consumer_key, self.consumer_secret)\n",
    "        auth.set_access_token(self.access_token, self.access_secret)\n",
    "        api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "        return api\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fee2290",
   "metadata": {},
   "source": [
    "#### Retrieve Data From the Tweepy Object Created\n",
    "Created a function that holds the logic to:\n",
    "* Create an authenticated api instance called **api** from the Tweepy object created above.\n",
    "* Retrieve the tweet_id values from the archive dataframe using  the numpy.values method, assing to **tweet_ids**.\n",
    "* Declare a **count** variable with the value of 0 to count the number of tweets retrieved.\n",
    "* Create an empty dictionary that will hold all the failed tweet_ids alongside the erro message.\n",
    "* Declare a timer object and assign it to the **start** variable.\n",
    "* Use the with context's open function to open a file (aliased it as **dump_file**).\n",
    "* Start a for loop inside the open function that does the following:\n",
    "    * Loops through all **tweet_ids** in the archive dataframe assigning every iteration to the variable **tweet_id**.\n",
    "    * Increments the **count** by 1 after every iteration.\n",
    "    * Prints out the **count** concatenated with the current **tweet_id** as strings.\n",
    "    * Opens a try clause to check for errors while retrieving data from Twitter:\n",
    "        * Call the get_status method from tweepy on the **api** object we created by passing in the **tweet_id** and setting\n",
    "          mode to extended in order to retrieve the entire untruncated text of the Tweet; then and assing it to a variable called **tweet**.\n",
    "        * Print 'Success' if a **tweet** was retrieved successfully.\n",
    "        * Convert the **tweet** as json dump.\n",
    "        * Write to the **dump_file** created earlier adding a line after each **tweet** object.\n",
    "    * Opens an except clause that raises an exception from tweepy.\n",
    "    * Print 'failed' whenever there's an error.\n",
    "    * Add the error into the **fails_dict** created earlier.\n",
    "* Create another timer instance called **end**.\n",
    "* Print the result of **end**-**start** (The time used to retrieve the tweets).\n",
    "* Print out the values from the **fails_dict**.\n",
    "\n",
    "Call the get_tweets function to start the process.\n",
    "         \n",
    "**Deliverables**:\n",
    "* A tweet_json_txt file with all the tweets downloaded. (2356 tweets)\n",
    "* The number of minutes it took to retrieve the tweets. (about 10 minutes)\n",
    "* A print out of all the failed tweets. (31 errors)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb6e7ae",
   "metadata": {},
   "source": [
    "#### Function to get tweets\n",
    "```\n",
    "def get_tweets():\n",
    "    tweet_ids = archive.tweet_id.values\n",
    "    api = Tweepy().authenticate()\n",
    "    count = 0\n",
    "    fails_dict = {}\n",
    "    start = timer()\n",
    "    \n",
    "    with open('tweet_json.txt', 'w') as dump_file:\n",
    "        for tweet_id in tweet_ids:\n",
    "            count +=1\n",
    "            print(str(count) + '+'+ str(tweet_id))\n",
    "            \n",
    "            try:\n",
    "                tweet = api.get_status(tweet_id, tweet_mode='extended')\n",
    "                print('Success')\n",
    "                json.dump(tweet._json, dump_file)\n",
    "                dump_file.write('\\n')\n",
    "            except tweepy.TweepError as err:\n",
    "                print('failed')\n",
    "                fails_dict[tweet_id] = err\n",
    "                pass\n",
    "            \n",
    "    end = timer()\n",
    "    print(end - start)\n",
    "    print(len(fails_dict))\n",
    "    print(fails_dict)\n",
    "get_tweets()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a35ed27",
   "metadata": {},
   "source": [
    " #### Read Data From tweet_json.txt\n",
    "I used the pd.read_json function to read all the data into a dataframe in order to display all the columns in the dataframe and choose the ones to use in the analysis. There are 32 columns in the dataset but I will select a handful of them since the rest are in the archive dataframe.\n",
    "\n",
    "I write a function called **json_to_df()** to read the data by directly selecting the columns I'm interested in. The function:\n",
    "* Declares and initializes an empty list called **tweet_list**.\n",
    "* Opens a try clause and uses a context to open the txt file created in the previous step.\n",
    "* Passes the file name and a 'utf-8' encoding into the context and assigns the file an alias of **f**.\n",
    "* Uses a list comprehension to loop through all lines inside **f** while parsing and converting them to a pythn dictionary using **json.loads.\n",
    "* Opens a for loop that loops through the list of dictionaries previously created by list comprehension.\n",
    "* Selects 5 columns of interest.\n",
    "* Names them ensuring that columns common in both the archive and the api tables have the same name to allow for smooth merging later.\n",
    "* Appends the column names as keys and the iterating variable as values to the **tweet_list** list to create a list of dictionaries.\n",
    "* Raises a json.JSONDecodeError in case of an error.\n",
    "* Uses pd.DataFrame passing in **tweet_list** as the list and assigning it to **tweet_df**.\n",
    "* Returns the pandas dataframe.\n",
    "\n",
    "I call the **json_to_df** function on the variable **api_tweets** which will be used during the assessment phase.\n",
    "\n",
    "**Deliverables:\n",
    "* A pandas dataframe called **api_tweets**\n",
    "* No errors thrown.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f10947",
   "metadata": {},
   "source": [
    "#### Function to Extract and Convert Data into a Pandas Dataframe\n",
    "```\n",
    "def json_to_df():\n",
    "    tweet_list = []\n",
    "\n",
    "    try:\n",
    "        with open('tweet_json.txt', encoding='utf-8') as f:\n",
    "            dicts_lst = [json.loads(line) for line in f]\n",
    "\n",
    "            for dict_item in dicts_lst:\n",
    "                tweet_list.append({'tweet_id': dict_item['id'],\n",
    "                                   'lang': dict_item['lang'],\n",
    "                                   'retweet_count': int(dict_item['retweet_count']),\n",
    "                                   'favorite_count' : int(dict_item['favorite_count']),\n",
    "                                   'timestamp': dict_item['created_at']})\n",
    "    except json.JSONDecodeError as error:\n",
    "        raise error\n",
    "    tweet_df = pd.DataFrame(tweet_list)\n",
    "    return  tweet_df\n",
    "api_tweets = json_to_df()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364596a8",
   "metadata": {},
   "source": [
    "## Assessing Data\n",
    "\n",
    "In this phase, I will be looking to identify issues that fall into two dimensions of data assessing methods namely quality issues and tidiness issues. \n",
    "\n",
    "1. Data quality issues: Data that have quality issues have issues with content like missing, duplicate, or incorrect data. This is called dirty data.\n",
    "2. Lack of tidiness: Data that has specific structural issues that slow you down when cleaning and analyzing, visualizing, or modeling your data later\n",
    "\n",
    "### Archive Dataframe\n",
    "The WeRateDogs Twitter archive contains basic tweet data for all 5000+ of their tweets by August 1, 2017, but not everything.\n",
    "\n",
    "**archive table feature description**\n",
    "\n",
    "* **tweet_id**: The unique identifier of the tweet\n",
    "* **in_reply_to_status_id**: The identifier for a reply to a status\n",
    "* **in_reply_to_user_id**: The identifier for reply to a user\n",
    "* **timestamp**: The date and time the tweet was made\n",
    "* **source**: The device the tweet was posted from\n",
    "* **text**: The text of the tweet which includes the dog name and dog stage\n",
    "* **retweeted_status_id**: The identifier of a retweet to a status\n",
    "* **retweeted_status_user_id**: The identifier of a retweet to a user\n",
    "* **retweeted_status_timestamp**: The date and time of the retweet\n",
    "* **expanded_urls**: The link to the tweet (they have the tweet_id at the end)\n",
    "* **rating_numerator**: The rating given to the dog\n",
    "* **rating_denominator**: The denominator used for the rating the dogs\n",
    "* **name**: The name of the dog\n",
    "* **doggo**: A big pupper, usually older (according to the Dogtionary)\n",
    "* **floofer**: Any dog really (according to the Dogtionary)\n",
    "* **pupper**: A small doggo, usually younger (according to the Dogtionary)\n",
    "* **puppo**: A transistional phase between pupper and doggo (according to the Dogtionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809b3a9d",
   "metadata": {},
   "source": [
    "#### Assesing steps\n",
    "\n",
    "* list all the columns in the table: **```archive.columns```**\n",
    "* display more than five rows from th table: **```archive```**\n",
    "* query a row that has data from beyond 2017: **```archive.query('tweet_id == 666049248165822465')```**\n",
    "* filter a portion of the dataset that are retweets: **```archive[~archive.retweeted_status_id.isnull()]```**\n",
    "* display the sum of duplicates in the expanded_urls column: **```archive.expanded_urls.duplicated().sum()```**\n",
    "* more assessing of the expanded urls: **```archive.expanded_urls.value_counts()```**\n",
    "* view information about the features i.e datatypes and number of rows: **```archive.info```**\n",
    "* check the sum of null values: **```archive.isnull()```**\n",
    "* more assesing: **```archive.sample(40)```**\n",
    "* show the statistical summary of the rating numerator: **```archive.rating_numerator.describe()```**\n",
    "* show the statistical summary of the rating denominator: **```archive.rating_denominator.describe()```**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275846d2",
   "metadata": {},
   "source": [
    "### Predictions Dataframe\n",
    "A table full of image predictions (the top three only) alongside each tweet ID, image URL, and the image number that corresponded to the most confident prediction (numbered 1 to 4 since tweets can have up to four images).\n",
    "\n",
    "**Image predictions table feature description**\n",
    "\n",
    "* **tweet_id**: The unique identifier of the tweet\n",
    "* **jpg_url**: The url to the dog's image\n",
    "* **img_num**: The number of images used for the prediction\n",
    "* **p1**: The algorithm's #1 prediction for the image in the tweet\n",
    "* **p1_conf**: How confident the algorithm is in its #1 prediction\n",
    "* **p1_dog**: Whether or not the #1 prediction is a breed of dog\n",
    "* **p2**: The algorithm's second most likely prediction\n",
    "* **p2_conf**: How confident the algorithm is in its #2 prediction\n",
    "* **p2_dog** Whether or not the #2 prediction is a breed of dog\n",
    "* **p3**: The algorithm's second most likely prediction\n",
    "* **p3_conf**: How confident the algorithm is in its #2 prediction\n",
    "* **p3_dog** Whether or not the #2 prediction is a breed of dog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782c3cc5",
   "metadata": {},
   "source": [
    "#### Assesing steps\n",
    "\n",
    "* list all the columns in the table: **```predictions```**\n",
    "* view information about the features i.e datatypes and number of rows: **```predictions.info```**\n",
    "* more assesing: **```predictions.sample(10)```**\n",
    "* show the statistical summary of numerical values: **```predictions.describe()```***\n",
    "* display the sum of duplicates in the table: **```predictions.duplicated().sum()```**\n",
    "* check the sum of null values: **```predictions.isnull()```**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e84a42",
   "metadata": {},
   "source": [
    "### Api Tweets Dataframe\n",
    "\n",
    "Data queried from the Twitter API tha contains additional data corresponding to the data in the archive table; **retweet_count** and **favorite_count**.\n",
    "\n",
    "**api tweets table**\n",
    "\n",
    "* **tweet_id**: The unique identifier of the tweet\n",
    "* **timestamp**: The date the tweet was posted\n",
    "* **lang**: The language used in the tweet\n",
    "* **retweet_count**: If it's been retweeted, how many retweets?\n",
    "* **favorite_count**: How many likes does the tweet have?\n",
    "* **full_text**: The untruncated text of the tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da25a027",
   "metadata": {},
   "source": [
    "#### Assesing steps\n",
    "\n",
    "* list all the columns in the table: **```api_tweets```**\n",
    "* view information about the features i.e datatypes and number of rows: **```api_tweets.info```**\n",
    "* more assesing: **```api_tweets.sample(10)```**\n",
    "* show the statistical summary of numerical values: **```nums_api_tweets.describe()```***\n",
    "* display the sum of duplicates in the table: **```api_tweets.duplicated().sum()```**\n",
    "* check the sum of null values: **```api_tweets.isnull()```**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde02267",
   "metadata": {},
   "source": [
    "### Quality Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266891d8",
   "metadata": {},
   "source": [
    "#### archive table\n",
    "\n",
    "* [Object datatype for timestamp instead of datetime datatype](#Issue-#1)\n",
    "example: tweet_id 700847567345688576\n",
    "* [Some tweets are retweets](#Issue-#4)\n",
    "* [Erroneous integer datatype for tweet_id](#Issue-#8)\n",
    "* [Missing dog names misrepresented as None](#Issue-10)\n",
    "* [Missing dog stages misrepresented as None](#Issue-#6-&-#7)\n",
    "* [Non-logical names for dog in the name column, i.e quite, such, not](#Issue-#9)\n",
    "* [Some rating_denominators are not equal to 10](#Issue-#12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149c3b6a",
   "metadata": {},
   "source": [
    "#### image predictions table\n",
    "* [Inconsistent capitalization of names in p1, p2, and p3](#Issue-#11)\n",
    "* [Erroneous integer datatype for tweet_id](#Issue-#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8118a894",
   "metadata": {},
   "source": [
    "#### api_tweets table\n",
    "* [Erroneous datatype integer for tweet_id](#Issue-#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39066816",
   "metadata": {},
   "source": [
    "### Tidiness issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59938cae",
   "metadata": {},
   "source": [
    "* [Some of the expanded_urls rows in the archive table contain more than one url](#Issue-#3)\n",
    "* [The in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp columns are not needed for this analysis](#Issue-#5)\n",
    "* [The dog stages are spread across four columns; doggo, floofer, pupper, puppo](Issue-#6-&-#7)\n",
    "* [There are tweets dated before 2017 i.e 2015, 2016. (the image prediction file only covers 2017)](#Issue-#2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993c43a2",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb814ec0",
   "metadata": {},
   "source": [
    "#### Make copies of the datasets\n",
    "Before I start cleaning the data, I make copies of the original datasets using the pd.copy function.\n",
    "\n",
    "```\n",
    "tweets_clean = api_tweets.copy()\n",
    "archive_clean = archive.copy()\n",
    "predictions_clean = predictions.copy()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6d2e2f",
   "metadata": {},
   "source": [
    "In this phase, I proceed to clean the issues found in the all the datasets each one at a time.\n",
    "I will use these three steps in cleaning each issue:\n",
    "\n",
    "1. **Issue**: State the issue to be cleaned and the table it belongs to.\n",
    "2. **Define**: Define the steps to be taken to clean the issue stated.\n",
    "3. **Code**: Write code to perfom the cleaning.\n",
    "4. **Test**: Run a test code to confirm that the issue has been cleaned.\n",
    "\n",
    "The image prediction model contains dog images from 2017 only; therefore I need to filter everything else out if I want to use the predictions. Additionally, I am supposed to analyze ratings from original tweets only, no retweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed48069",
   "metadata": {},
   "source": [
    "### Issue #1\n",
    "\n",
    "Object datatype for timestamp instead of datetime datatype in the archive table\n",
    "\n",
    "#### Define\n",
    "Use pd.to_datetime to convert column values to datetime\n",
    "\n",
    "#### Code\n",
    "```\n",
    "archive_clean['timestamp'] = pd.to_datetime(archive_clean.timestamp)\n",
    "```\n",
    "\n",
    "#### Test\n",
    "```\n",
    "archive_clean.timestamp.dtype\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7769d10",
   "metadata": {},
   "source": [
    "### Issue #2\n",
    "\n",
    "**There are tweets dated before 2017 i.e 2015, 2016. (the image prediction file only covers 2017)\n",
    "example: tweet_id 700847567345688576**\n",
    "\n",
    "#### Define\n",
    "Filter and drop all tweets dated before 2017 in both archive and tweets table\n",
    "\n",
    "#### Code\n",
    "```\n",
    "not_2017 = archive_clean[archive_clean.timestamp.dt.year != 2017]\n",
    "archive_clean.drop(not_2017.index, inplace=True)\n",
    "```\n",
    "\n",
    "#### Test\n",
    "```\n",
    "archive_clean[archive_clean.timestamp.dt.year != 2017]\n",
    "archive_clean.query('tweet_id == 700847567345688576')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb198570",
   "metadata": {},
   "source": [
    "### Issue #3\n",
    "\n",
    "**Some of the expanded_urls rows in the archive table contain more than one url**\n",
    "\n",
    "#### Define\n",
    "* Ceate new urls by combining tweet status uris with corresponding tweet_ids and assign them to a new column: tweet_status_urls.\n",
    "* Then drop the expanded_urls column from the table\n",
    "\n",
    "#### Code\n",
    "```\n",
    "uri = 'https://twitter.com/dog_rates/status/'\n",
    "archive_clean['tweet_status_urls'] = archive_clean['tweet_id'].apply(lambda  row: '{}{}/photo/1'.format(uri,row))\n",
    "archive_clean.drop('expanded_urls', axis=1, inplace=True)\n",
    "```\n",
    "\n",
    "#### Test\n",
    "```\n",
    "archive_clean.columns\n",
    "len(archive_clean.tweet_status_urls.unique())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41cd711",
   "metadata": {},
   "source": [
    "### Issue #4\n",
    "\n",
    "**Some tweets are retweets**\n",
    "\n",
    "#### Define\n",
    "Filter and delete all tweets that the retweet_status_id is not NaN(these tweets also contain 'RT @') in the archive dataframe.\n",
    "\n",
    "#### Code\n",
    "```\n",
    "archive_clean= archive_clean[archive_clean.retweeted_status_id.isnull()]\n",
    "```\n",
    "\n",
    "#### Test\n",
    "```\n",
    "sum(~archive_clean.retweeted_status_id.isnull())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abac8a1d",
   "metadata": {},
   "source": [
    "### Issue #5\n",
    "\n",
    "* in_reply_to_status_id\n",
    "* in_reply_to_user_id\n",
    "* retweeted_status_id\n",
    "* retweeted_status_user_id\n",
    "* retweeted_status_timestamp\n",
    "are columns not needed for this analysis. Additionally, they hava missing values of about 90%\n",
    "\n",
    "#### Define\n",
    "Drop these columns from the archive table\n",
    "\n",
    "#### Code\n",
    "```\n",
    "out_of_scope = ['in_reply_to_status_id', 'in_reply_to_user_id', 'retweeted_status_user_id', 'retweeted_status_id', 'retweeted_status_timestamp']\n",
    "archive_clean.drop(out_of_scope, axis=1, inplace=True)\n",
    "```\n",
    "\n",
    "#### Test\n",
    "```\n",
    "archive_clean.columns\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aed6bff",
   "metadata": {},
   "source": [
    "### Issue #6 & #7\n",
    "**The dog stages are spread across four columns; doggo, floofer, pupper, puppo instead just one**:*\n",
    "\n",
    "**Missing dog stages in the archive table misrepresented as None**\n",
    "\n",
    "#### Define\n",
    "* Use Regex to findall on the text column to find dog stages and fill the stage column with those values.\n",
    "* Replace missing values with np.nan (NaN)\n",
    "* Drop the the four columns from the table.\n",
    "\n",
    "#### Code\n",
    "```\n",
    "stages = ['doggo', 'floofer', 'pupper', 'puppo' ]\n",
    "pattern = r\"(?=(\\b\" + '|'.join(stages) + r\"\\b))\"\n",
    "archive_clean[\"stage\"] = archive_clean.text.str.findall(pattern).str.join(',').replace({'':np.nan})\n",
    "archive_clean.drop(stages, axis=1, inplace=True)\n",
    "```\n",
    "\n",
    "#### Test\n",
    "```\n",
    "archive_clean.columns\n",
    "archive_clean.stage.value_counts()\n",
    "archive_clean.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782b3177",
   "metadata": {},
   "source": [
    "### Issue #8\n",
    "\n",
    "**Erroneous tweet_id misrepresented as integer in all three tables**\n",
    "\n",
    "#### Define\n",
    "Convert tweet_id to string datatype using pandas .astype() function\n",
    "\n",
    "#### Code\n",
    "```\n",
    "archive_clean.tweet_id = archive_clean.tweet_id.astype(str)\n",
    "tweets_clean.tweet_id = tweets_clean.tweet_id.astype(str)\n",
    "predictions_clean.tweet_id = predictions_clean.tweet_id.astype(str)\n",
    "```\n",
    "\n",
    "#### Test\n",
    "```\n",
    "archive_clean.tweet_id.dtype\n",
    "tweets_clean.tweet_id.dtype\n",
    "predictions_clean.tweet_id.dtype\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152f549d",
   "metadata": {},
   "source": [
    "### Issue #9\n",
    "\n",
    "**Non-logical names in the archive table i.e quite, such, a, not, one**\n",
    "\n",
    "#### Define\n",
    "Filter out non-logical names; they seem to all be lowercased.\n",
    "\n",
    "#### Code\n",
    "```\n",
    "non_logical = []\n",
    "_ = [non_logical.append(i) for i in archive_clean.name if i.islower()]\n",
    "archive_clean = archive_clean[~archive_clean.name.str.islower()]\n",
    "```\n",
    "\n",
    "#### Test\n",
    "```\n",
    "archive_clean[archive_clean.name.str.islower()]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8429b5b",
   "metadata": {},
   "source": [
    "### Issue #10\n",
    "\n",
    "**Missing dog names in the archive table misrepresented as None**\n",
    "\n",
    "#### Define\n",
    "Use np.nan to replace 'None' with NaN\n",
    "\n",
    "#### Code\n",
    "```\n",
    "archive_clean.name = archive_clean.name.replace('None', np.nan)\n",
    "```\n",
    "\n",
    "#### Test\n",
    "```\n",
    "archive_clean[archive_clean.name.isnull()]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af93532e",
   "metadata": {},
   "source": [
    "### Issue #11\n",
    "\n",
    "**Inconsistent capitalization of names in p1, p2, and p3 in the predictions table**\n",
    "\n",
    "#### Define\n",
    "Use the capitalize function to capitalize every first letter in the columns\n",
    "\n",
    "#### Code\n",
    "```\n",
    "breeds = ['p1', 'p2', 'p3']\n",
    "predictions_clean[breeds] = predictions_clean[breeds].apply(lambda x: x.str.capitalize())\n",
    "```\n",
    "\n",
    "#### Test\n",
    "```\n",
    "predictions_clean[~predictions_clean.p1.apply(lambda x: x[0].isupper())]\n",
    "predictions_clean[~predictions_clean.p2.apply(lambda x: x[0].isupper())]\n",
    "predictions_clean[~predictions_clean.p3.apply(lambda x: x[0].isupper())]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d686d0",
   "metadata": {},
   "source": [
    "### Issue #12\n",
    "\n",
    "**Rating denominator in the archives table is lower or higher than 10**\n",
    "\n",
    "#### Define\n",
    "Delete all tweets that with rating denominator that is not equal to 10\n",
    "\n",
    "#### Code\n",
    "```\n",
    "archive_clean = archive_clean[archive_clean.rating_denominator == 10]\n",
    "```\n",
    "\n",
    "#### Test\n",
    "```\n",
    "archive_clean[archive_clean.rating_denominator !=10]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8bbc38",
   "metadata": {},
   "source": [
    "### Deliverables:\n",
    "* a clean archive table with 390 rows and 9 columns\n",
    "* a clean predictions table with 2075 rows and 12 columns\n",
    "* a clean api table with 2325 rows and 4 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94db3402",
   "metadata": {},
   "source": [
    "## Storing Data\n",
    "\n",
    "I will merge and save gathered, assessed, and cleaned master dataset to a CSV file named \"twitter_archive_master.csv\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4c7fa7",
   "metadata": {},
   "source": [
    "###  Merge the three dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba663f79",
   "metadata": {},
   "source": [
    "```\n",
    " twitter_archive_master = archive_clean.merge(tweets_clean,on='tweet_id').merge(predictions_clean,on='tweet_id')\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58de716b",
   "metadata": {},
   "source": [
    "### Export the dataframe to a csv file using pd.to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301e0b4e",
   "metadata": {},
   "source": [
    "```\n",
    "twitter_archive_master.to_csv('twitter_archive_master.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b2cf91",
   "metadata": {},
   "source": [
    "## Regards\n",
    "Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "380.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
